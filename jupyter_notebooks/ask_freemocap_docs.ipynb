{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFTi1HFe3Vxf"
      },
      "source": [
        "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jgVeBh73WBf"
      },
      "source": [
        "This notebook accompanies [this YouTube video](https://www.youtube.com/watch?v=zaYTXQFR0_s)\n",
        "walking through what LangChain is and interviewing the creator, Harrison Chase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8Loy7nBuu-8"
      },
      "source": [
        "## Auth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQFKCcwUDkCX",
        "outputId": "6aebf7b9-234d-443d-f6b3-75a6a8cd12cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qqq langchain openai\n",
        "%pip install -qqq beautifulsoup4\n",
        "%pip install -qqq unstructured\n",
        "%pip install -qqq tiktoken\n",
        "%pip install -qqq faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lTk21-dEYyy",
        "outputId": "356ef28a-a043-425c-9263-29fa9256fd2b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "#set up for tracing\n",
        "os.environ[\"LANGCHAIN_HANDLER\"] = \"langchain\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "9scp87XTHp1r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content=\"About Us\\n\\nThe Free Motion Capture Project (FreeMoCap) aims to provide research-grade markerless motion capture software to everyone for free.\\n\\nWe're building a user-friendly framework that connects an array of bleeding edge open-source tools from the computer vision and machine learning communities to accurately record full-body 3D movement of humans, animals, robots, and other objects.\\n\\nWe want to make the newly emerging mind-boggling, future-shaping technologies that drive FreeMoCap's core functionality accessible to communities of people who stand to benefit from them.\\n\\nWe follow a â€œUniversal Designâ€ development philosophy, with the goal of creating a system that serves the needs of a professional research scientist while remaining intuitive to a 13-year-old with no technical training and no outside assistance.\\n\\nA high-quality, minimal-cost motion capture system would be a transformative tool for a wide range of communities - including 3d animators, game designers, athletes, coaches, performers, scientists, engineers, clinicians, and doctors. We hope to create a system that brings new technological capacity to these groups while also building bridges between them.\\n\\nWe want to help them do it\\n\\nâœ¨ðŸ’€âœ¨\\n\\n(copied from https://freemocap.org)\", metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\about_us.md'}), Document(page_content=\"FAQ's - Ongoing Draft\\n\\nWork in progress, you can find the WIP version of this page on the github repo (but no promises on its accuracy)\", metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\frequently_asked_questions_faq.md'}), Document(page_content='Welcome Skele-Friend! ðŸ’€âœ¨\\n\\nThis is the official and most up-to-date place to find documentation for FreeMoCap. We\\'re slowly building a Knowledge Base that roughly follows the \\'diataxis framework\\'. Our documentation is very much a work in progress, so we appreciate your patience, support, and engagement!\\n\\nIf you\\'re looking for a quick start, head on over to our \"How to\" Guides page!\\n\\n??? info \"These docs are a work in progress! Better tutorials/walkthroughs/etc coming soon!\"\\n\\nHelpful Links\\n\\nThe FreeMoCap Website https://freemocap.org\\n\\nThe FreeMoCap GitHub https://github.com/freemocap/freemocap\\n\\nSupport FreeMoCap by donating to our non-profit that supports our work!\\n\\nTroubleshooting?\\n\\nIf you run into an issue using the software itself, post an issue on our GitHub, here: https://github.com/freemocap/freemocap/issues\\n\\nIf there\\'s an error in our documentation, post an issue on our documentation repository, here: https://github.com/freemocap/documentation/issues\\n\\nJoin the Discord and ask a question in the #help-requests channel.\\n\\nClick here to join our Discord', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\index.md'}), Document(page_content='Privacy Policy\\n\\nWe respect and value your privacy. This Privacy Policy outlines how we collect, use, and protect any personal or anonymous data we may collect from users of our software. By using our software, you agree to the terms of this Privacy Policy. \"We\" refers to the FreeMoCap development team, which maintains the FreeMoCap software. \"You\" refers to the user of our software.\\n\\nUser data helps us understand how we can make our software better for you and allows us to demonstrate to agencies and corporations that people are using our software, which may help us to grow the project in the future.\\n\\nCollection of Anonymous User Data\\n\\nIf you check the \"Send anonymous usage information\" check box on the main page of the GUI, we will collect anonymous user data when you use our software. This information is sent to us as a \"ping\" of data posted to pipedream every time the software is used.\\n\\nThe data we currently collect includes:\\n\\nyour IP Address (because the \"ping\" is sent to pipedream via a POST request, we cannot avoid collecting your IP address)\\n\\nThe time the \"ping\" was sent\\n\\ninformation about your cameras\\n\\nYou can view the code that collects this data here.\\n\\nIf you do not wish to share your data, you may turn off \"pings\" at any point in time. To turn off user pings, uncheck the \"Send anonymous usage information\" box on the home screen of the FreeMoCap GUI.\\n\\nProtection of User Data\\n\\nWe take the protection of your data seriously and will not sell or distribute any personal or anonymous data we collect to third parties, except as required by law. We use industry-standard security measures to protect your data from unauthorized access, use, or disclosure. Currently, only core FreeMoCap developers (Jonathan Matthis, and Trenton Wirth) have access to the user data we have collected.\\n\\nYour Control Over Your Data\\n\\nAs a user of our software, you have control over your data. You can choose to turn off \"pings\" at any point in time by unchecking the \"Send anonymous usage information\" box on the home screen of the FreeMoCap GUI.\\n\\nIf you wish to have your user data deleted, you may contact us at info AT freemocap DOT org.\\n\\nUpdates to This Privacy Policy\\n\\nWe may update this Privacy Policy as our project evolves, so please check back periodically for changes. Your continued use of our software after any changes to this Privacy Policy will constitute your acceptance of such changes.\\n\\nContact Us\\n\\nIf you have any questions or concerns about this Privacy Policy or our use of your data, please contact us at info AT freemocap DOT org, or reach out to us on our Discord.', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\privacy_policy.md'}), Document(page_content=\"Python Reference\\n\\nThis'll be like, I don't know, the API or something?\", metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\python_reference.md'}), Document(page_content='Troubleshooting\\n\\nFor fixing specific problems\\n\\nInstallation problems\\n\\nEtc', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\troubleshooting.md'}), Document(page_content='How to install and run the alpha GUI\\n\\n!!! warning \"This is a work-in-progress :D\"\\n\\nMost of the dev team runs the GUI through PyCharm, but its easier to write instructions on how to run from an anaconda prompt.\\n\\nPre-requisites:\\n\\nInstall Anaconda\\nhttps://anaconda.org\\n\\nInstall git (just use the defaults)\\n\\n(OPTIONAL, but highly recommended) Install Blender - https://blender.org\\n\\nInstallation instructions\\n\\nOpen anaconda enabled terminal\\n\\nCreate a python=3.9 environment\\nbash\\nconda create -n freemocap-gui python=3.9\\n\\nActivate that environment:\\nconda activate freemocap-gui\\n\\nClone the repository (i.e. download the code from github. It\\'ll show up in the current working directory of your terminal session)\\ngit clone https://github.com/freemocap/freemocap\\n\\nNavigate into that newly cloned/downloaded freemocap folder with:\\ncd freemocap\\n\\nInstall the dependencies listed in the requirements.txt file:\\npip install -r requirements.txt\\n\\nRun the GUI by running the src/gui/main/main.py file by entering this command into the terminal:\\nbash\\npython src/gui/main/main.py\\n\\nHopefully a GUI popped up! There are no docs on usage yet, so just click and see what you can figure out :joy:\\n\\nCurrent limitations\\n\\nAt the moment, the alpha GUI\\'s method for connecting to the cameras is very innefficient and will experience framerate drops with more than ~3 cameras (even with a powerful PC). We\\'re working on a fix, and should have it handled soon! In the mean time, you can still use the GUI to process videos recorded with other methods (workflow described in the next section!)', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\how_to_guides\\\\how_to_install_and_run_the_alpha_gui.md'}), Document(page_content='How to process pre-recorded synchronized videos with alpha GUI\\n\\nThis is a workflow you can use to process any pre-recorded & synchronized videos.\\n\\n??? take-note \"Note: Rebooting the GUI\"\\n\\n!!! tip-full-width \"Follow for pre-alpha Tips\"\\n\\nStep 1 - Run the GUI\\n\\n??? Info \"One-click installation coming soon\"\\n\\nMake sure you\\'re using the most recent version of the freemocap software by entering the command git pull in the terminal before running main.py\\n\\nPrepare a python environment using these instructions fromREADME on the freemocap GitHub repository.\\n\\nOnce you have set up and activated your environment, run  python src/gui/main/main.py in a terminal with the proper environment activated.\\n\\nA GUI should pop up that looks like this:\\n\\nStep 2 - Import Videos\\n\\nSelect Import Synchronized Videos (or ctrl+I).\\n\\n2.1 - Create session_id\\n\\n!!! take-note inline end \"The terminal is your friend!\"\\n\\nThis session_id is created based on the date and time that you began this session (format: session_YYYY-MM-DD_HH_MM_SS) will be the name of the folder that will house the data from this session. It will be located in your user directory in a folder called freemocap_data/[session_id]\\n\\nOptional - add a tag to the session_id dialog to identify which videos you will import.\\n\\n!!! warning \"Warning: Do not use spaces in the session_id, use underscores _ or dashes - to break up words instead.\"\\n\\n2.2 - Select folder of synchronized videos\\n\\n!!! tip-full-width \"pre-alpha tip\"\\n\\nClick Select set of synchronized videos... button.\\n\\nSelect the folder full of synchronized (.mp4) videos.\\n\\n??? info \"Converting videos to .mp4\"\\n\\n??? info \"Synchronize your own videos\"\\n\\nStep 3 - Calibration\\n\\nOpen the 2-Capture Volume tab.\\n\\nOption #1 - Load the camera_calibration.toml from a prior freemocap session\\n\\n!!! tip-full-width \"pre-alpha tip\"\\n\\nSelect Load Camera Calibration .toml file... option.\\n\\nLoad in the ...camera_calibration.toml file from a previously recorded session.\\n\\nOption #2 - Process calibration videos in GUI\\n\\nWith Calibrate \\'from synchronized_videos\\' folder option selected, click Calibration Capture Volume from Videos button.\\n\\n??? Warning \"Warning: Only select Option #2 if your videos include a Charuco Board calibration procedure.\"\\n\\nStep 4 - Process Videos\\n\\nOpen the 3-Motion Capture Data tab.\\n\\nSet processing parameters however you like.\\n\\n??? blender \"Recommended - Install Blender\"\\n\\n??? Warning \"Warning: You may need to manually specify the location of the Blender executable in the GUI.\"\\n\\nClick Process All Steps Below button.\\n\\nClicking this button will automatically run through every other option on this tab, and you wont need to click anything else!\\n\\nStep 5 - Visualize Recording\\n\\nIf all went well, the GUI may have automatically opened Blender with your motion capture data pre-loaded, and you should be done!\\nIf not, double check to make sure the blender executable location is specified correctly.\\nYou can re-launch the Export to Blender process with the Generate \\'.blend\\' file button.\\n\\n!!! finished \"You\\'re Done! Next Steps?\"', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\how_to_guides\\\\how_to_process_previously_recorded_videos.md'}), Document(page_content='How to use the pre-alpha\\n\\n!!! info \"We\\'re in the process of launching our alpha...\"\\n\\nInstalling the pre-alpha\\n\\n!!! take-note \"Note: Our pre-alpha is frozen\"\\n\\nOpen an Anaconda-enabled command prompt or powershell window and enter the following commands:\\n\\n1) Create a Python 3.7 Anaconda environment\\nbash \\nconda create -n freemocap-env python=3.7\\n\\n2) Activate that newly created environment\\nbash\\nconda activate freemocap-env\\n3) Install freemocap (version 0.0.54)  from PyPi using pip\\nbash\\npip install freemocap==0.0.54\\n\\n!!! warning \"Warning: BUG FIX - Update mediapipe with pip install mediapipe --upgrade\"\\n\\nThat should be it!\\n\\nHow to create a new pre-alpha recording session\\n\\ntl;dr- Activate the freemocap Python environment and run the following lines of code (either in a script or in a console)\\n\\npython\\nimport freemocap\\nfreemocap.RunMe()\\n\\n!!! blender \"Be cool, use Blender\"\\n\\nFor additional, more detailed instructions (including methods to re-process recorded sessions), refer to the OLD_README.md document)', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\how_to_guides\\\\how_to_use_the_pre-alpha_code.md'}), Document(page_content='\"How to\" Guides\\n\\nText and pictures\\n\\nHow to use the pre-alpha\\n\\nHow to install and run the alpha GUI\\n\\nHow to process pre-recorded synchronized videos with alpha GUI\\n\\nLinks to videos\\n\\nHow to choose your cameras\\n\\nHow to calibrate your capture volume', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\how_to_guides\\\\index.md'}), Document(page_content='comments: true\\n\\nRoadmap\\n\\nIn preparation for v0.1.0 alpha GUI release\\n\\n[ ] Fix the Blender output\\n\\nProbably going to build something based on the methods developed in @cgtinker\\'s BlendArMocap add-on\\nlink to notes on this topic\\n\\n[ ] Fix the cameras\\n\\nalpha GUI starts lagging after about 3 cameras on a fast PC because cameras are run by threads (which compete for CPU with the GUI, vs the pre-alpha where it was all Terminal)\\nGonna implement a smarter method of connecting to cameras that involves splitting them across separate processes\\ngot some promising stuff on freemocap/fast-camera-capture, it just needs some more testing and then we can implement it in the GUI\\n\\nPlanned work after v0.1.0 alpha GUI release\\n\\n[ ] UI/UX Workflow improvements\\n\\n[ ] Improve and validate the post processing pipeline\\nImprove, streamline and develop validations/tests/diagnostics for: \\nGap-filling \\ncurrent method - Linear interpolation\\n\\n\\nFiltering \\ncurrent method - Zero-lag 4th order low-pass Butterworth filter with cut off at, like, 7Hz\\n\\n\\nOrigin Align \\ncurrent method - a variety of things that don\\'t work as well as I\\'d like, lol\\nTry to automatically place skeleton on origin with feet on \\'ground\\' (i.e. the XY plane)\\n\\n[ ] Add automated test suite (via GitHub Actions)\\n[ ] for folder structure (e.g. \\'completeness\\')\\n[ ] for accuracy of post processing pipeline (e.g. \\'kinematic diagnostics\\')\\n[ ] for camera recording quality (e.g. \\'timestamp diagnostics\\')\\n\\n[ ] New architecture for TUI Mothership method to develop alongside the standalone QtGUI\\nSimple textual/TUI mothership terminal based app that launches qt wizards for each recording/processing stage\\nBasically:\\nit\\'s just a simple launcher\\nwe can ask contributors to develop add-ons/plug-ins/improvements via QWidget objects that can launch from a widget.show() method. \\nThat way we can test/develop new capactites and organize them via the TUI interface before trying to integrate into the core freemocap GUI\\nKinda like a \\'plug-in\\' system\\nKinda like pyqthgraph\\'s examples thing\\n\\n\\nKeeps things nice and modular and easy to develop/collaborate\\nLets higher XP folks make tools that Lower XP folks can use/test/learn!!\\n\\nThe Road to Real-time\\n\\nDesired workflow\\n\\n!!! take-note \"NOTE - This is the DESIRED workflow, not the current one lol :joy:\"\\n\\nInstall and launch FreeMoCap software\\n\\nGo to freemocap.org/downloads\\n\\nDownload and install the latest version of FreeMoCap for your OS\\n\\nRun installation wizard\\n\\nDouble click on Skelly icon and launch the GUI\\n\\nSet up Cameras\\n\\nPlug them into the PC\\n\\nSet them up so they are pointing at the subject with sufficient lighting and overlapping fields of view\\n\\nCalibrate capture volume\\n\\nShow charuco board to each camera, each camera has shared views of the same board with at least one other cameras\\n\\nPress \\'record/stream\\' or whatever\\n\\nA little thing pops up with info/diagnostics about the stream (IP address, success rate, framerate on both sides, system load info, etc)\\n\\nConnect to another 3d software (say Blender)\\n\\nYou see a rigged humanoid mesh pop up in a 3d viewport and the skeleton matches your movement (with max possible accuracy and min possible latency)\\n\\nCapacities needed\\n\\nInstall\\n\\nstandalone installers for Windows, Mac and Linux\\n\\ndownloadable from freemocap website\\n\\nauto-build and update versions via Github Actions\\n\\nCore software\\n\\nimprove workflow and UX as much as possible\\n\\nclean and sanitize skeleton output as much as possible\\n\\nminimize/optimize/distribute computational load as much as possible (and provide users with \\'Speed vs Accuracy\\' knob based on their needs (and try to automate this process like they do in video game?))\\n\\nHandle camera synchronization/processing in real-time\\n\\nFormat skeleton in some intelligent way and stuff it into a Queue/PIPE that connects to a socket or something\\n\\nExternal apps\\n\\nBlender/Unreal/Unity/Gadot (or whatever) opens a socket and listens for skeleton data\\n\\nSkeleton data drives some 3d human skeleton/model/avatar/whatever\\n\\nAnd that\\'s realtime baybee :sunglasses: :sparkles:\\n\\nThat\\'s the dream, anyway.\\n\\nExpected timeline\\n\\n\"Soon\" Â¯\\\\_(ãƒ„)_/Â¯', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\roadmap\\\\roadmap.md'}), Document(page_content=\"freemocap blender addon\\n\\nStrategy\\n\\nBased on fork of - https://github.com/cgtinker/blendarmocap\\n\\nfigure out what was happening on freemocap branch of above\\n\\nRevert to main branch, then build new freemocap-auto-blender-output script based on main.py\\n\\nTO DO \\n- [ ] auto-download sample data\\n- [x] why skelly face down? \\n    - b/c he swaps XYZ -> -XZ-Y in the body, hand, face _processing.py files\\n- [-] don't load live\\n    - can replaceload_freemocap with better version (the existing one hijacks the holistic processor, and that's not necessary)\\n- [ ] load body hands and face separately\\n- [ ] bind other parts of rig to data\\n- [ ] Remove hard-coded numbers in the processing functions  e.g. pose_processing.py Line214\", metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\roadmap\\\\notes\\\\2022-11-13_blender_addon_notes.md'}), Document(page_content='Terminology\\n\\nCapture Volume\\n\\n3-dimensional area (volume) with sufficient camera coverage to support 3D tracking.\\n\\nCalibration\\n\\nLink to a section of the \\'braindump\\' video discussing capture volume calibration\\n\\nCharuco Board\\n\\nLink to a section of the \\'braindump\\' video discussing capture volume calibration\\n\\nMediapipe\\n\\nhttps://google.github.io/mediapipe/solutions/holistic\\n\\nProcessing Stages\\n\\nThis is a brief description of each of the processing \\'stages\\' necessary to use a bunch of USB webcams to reconstruct the 3D kinematic (i.e. mocap) data of the human subject!\\n\\nSome parts refer to the folder and function names of the pre-alpha version of the code, but the concepts are mostly the same in the alpha version.\\n\\nStage 1 - Record Videos\\n\\nRecord raw videos from attached USB webcams and timestamps for each frame\\n\\nRaw Videos saved to FreeMoCap_Data/[Session Folder]/RawVideos\\n\\nStage 2 - Synchronize Videos\\n\\nUse recorded timestamps to re-save raw videos as synchronized videos (same start and end and same number of frames). Videos saved to\\n\\nSynchronized Videos saved to FreeMoCap_Data/[Session Folder]/SynchedVideos\\n\\nStage 3 - Calibrate Capture Volume\\n\\nUse Anipose\\'s Charuco-based calibration method to determine the location of each camera during a recording session and calibrate the capture volume\\n\\nCalibration info saved to [sessionID]_calibration.toml and [sessionID]_calibration.pickle\\n\\nStage 4 - Track 2D points in videos and Reconstruct 3D <-This is where the magic happens âœ¨\\n\\nApply user specified tracking algorithms to Synchronized videos (currently supporting MediaPipe, OpenPose, and DeepLabCut) to generate 2D data \\nSave to FreeMoCap_Data/[Session Folder]/DataArrays/ folder (e.g. mediaPipeData_2d.npy)\\n\\n\\nCombine 2d data from each camera with calibration data from Stage 3 to reconstruct the 3d trajectory of each tracked point\\nSave to /DataArrays folder (e.g. openPoseSkel_3d.npy)\\n\\n\\nNOTE - you might think it would make sense to separate the 2d tracking and 3d reconstruction into different stages, but the way the code is currently set up it\\'s cleaner to combine them into the same processing stage Â¯\\\\_(ãƒ„)_/Â¯\\n\\nStage 5 - Use Blender to generate output data files (optional, requires Blender installed. set freemocap.RunMe(useBlender=True) to use)\\n\\nHijack a user-installed version of Blender to format raw mocap data into  a .blend file including the raw data as keyframed emtpies with a (sloppy,  inexpertly) rigged and meshed armatured based on the Rigify Human Metarig\\nSave .blend file to [Session_Folder]/[Session_ID]/[Session_ID].blend \\nYou can double click that .blend file to open it in Blender. \\nFor instructions on how to navigate a Blender Scene, try this YouTube Tutorial\\n\\nStage 6 - Save Skeleton Animation!\\n\\nCreate a Matplotlib based output animation video.\\nSaves Animation video to: [Session Folder]/[SessionID]_animVid.mp4\\nNote - This part takes for-EVER ðŸ˜…\\n\\nReprojection Error\\n\\n\"Reprojection error\" is the distance (in pixels?) between the originally measured point (i.e. the 2d skeleton) and the reconstructed 3d point reprojected back onto the image plane.\\n\\nThe intuition is that if the 3d reconstruction and original 2d track are perfect, then reprojection error will be Zero. If it isn\\'t, then there is some inaccurate in either:\\n\\nthe original 2d tracks (i.e. bad skeleton detection in one or more cameras),\\n\\nin the 3d reconstruction (i.e. bad camera calibration),\\n\\na combination of the two\\n\\nClick here to follow a conversation about reprojection error on discord', metadata={'source': 'C:\\\\Users\\\\jonma\\\\github_repos\\\\freemocap_organization\\\\documentation\\\\docs\\\\terminology\\\\terminology.md'})]\n"
          ]
        }
      ],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "loader = DirectoryLoader(r'C:\\Users\\jonma\\github_repos\\freemocap_organization\\documentation\\docs', glob=\"**/*.md\")\n",
        "documents = loader.load()\n",
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "docsearch = FAISS.from_documents(documents, embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3xX9KUF1KxG"
      },
      "source": [
        "### Ask questions and get answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hiuGx6cqH8cd"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
        "\n",
        "\n",
        "\n",
        "chain = load_qa_with_sources_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu96kQNy1SFi",
        "outputId": "cb00330f-be19-4646-8c48-a862ee8d4f51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FreeMoCap is a project that aims to provide research-grade markerless\n",
            "motion capture software for free. It connects various open-source\n",
            "tools from the computer vision and machine learning communities to\n",
            "accurately record full-body 3D movement of humans, animals, robots,\n",
            "and other objects. The current documentation does not mention real-\n",
            "time capacity. A charuco board is used for calibrating the capture\n",
            "volume in the motion capture process. The creators of FreeMoCap are\n",
            "not explicitly mentioned in the provided content.\n",
            "\n",
            "SOURCES:\n",
            "C:\\Users\\jo\n",
            "nma\\github_repos\\freemocap_organization\\documentation\\docs\\about_us.md\n",
            ", C:\\Users\\jonma\\github_repos\\freemocap_organization\\documentation\\doc\n",
            "s\\terminology\\terminology.md\n"
          ]
        }
      ],
      "source": [
        "query = \"What is FreeMoCap? Does it have realtime capacity? Who made it? What is a charuco board?\"\n",
        "# query = \"What is LangChainHub?\"\n",
        "# query = \"Does LangChain integrate with OpenAI? If so, how?\"\n",
        "\n",
        "docs = docsearch.similarity_search(query)\n",
        "result = chain({\"input_documents\": docs, \"question\": query})\n",
        "\n",
        "text = \"\\n\".join(textwrap.wrap(result[\"output_text\"]))\n",
        "text = \"\\n\\nSOURCES:\\n\".join(map(lambda s: s.strip(), text.split(\"SOURCES:\")))\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7ca9p3o2Cqz",
        "outputId": "0620b36d-e2b2-4db2-8330-03f97d8ff434"
      },
      "outputs": [],
      "source": [
        "print(*textwrap.wrap(result[\"input_documents\"][0].page_content), sep=\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
